{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTFQveNECBozEDZUhiSR7V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raichalvarghese/Data_Science_Lab/blob/master/Generative_Adversarial_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ifIQY8p1KmG0"
      },
      "outputs": [],
      "source": [
        "from numpy import zeros, ones, expand_dims, asarray\n",
        "from numpy.random import randn, randint\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
        "from keras.layers import LeakyReLU, Dropout, Embedding\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras import initializers\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, _), (_, _) = fashion_mnist.load_data()\n",
        "X_train = X_train.astype(np.float32) / 127.5 - 1\n",
        "X_train = np.expand_dims(X_train, axis=3)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DCS3f6uM9Th",
        "outputId": "6743963f-b131-41eb-b447-377c4d6d4413"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    x_input = randn(latent_dim * n_samples)  \n",
        "    z_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return z_input"
      ],
      "metadata": {
        "id": "4FgajDASM_xg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(X_train, n_samples):\n",
        "    ix = randint(0, X_train.shape[0], n_samples) \n",
        "    X = X_train[ix]  \n",
        "    y = ones((n_samples, 1)) \n",
        "    return X, y"
      ],
      "metadata": {
        "id": "2VXVgl4nM_pe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    z_input = generate_latent_points(latent_dim, n_samples)\n",
        "    images = generator.predict(z_input)  \n",
        "    y = zeros((n_samples, 1))\n",
        "    return images, y"
      ],
      "metadata": {
        "id": "GuPBC8PZND0Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
        "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "    X = (X + 1) / 2.0\n",
        "    for i in range(100):\n",
        "        pyplot.subplot(10, 10, 1 + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "    filename2 = 'model_%04d.h5' % (step+1)\n",
        "    g_model.save(filename2)\n",
        "    print('>Saved: %s' % (filename2))"
      ],
      "metadata": {
        "id": "bcU5tV8TND2B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_plot(examples, n_examples):\n",
        "    for i in range(n_examples):\n",
        "        pyplot.subplot(sqrt(n_examples), sqrt(n_examples), 1 + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "    pyplot.show()"
      ],
      "metadata": {
        "id": "SSJJMvtpND5j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_discriminator(in_shape=(28, 28, 1)):\n",
        "    init = RandomNormal(stddev=0.02)  \n",
        "    in_image = Input(shape=in_shape)\n",
        "    fe = Flatten()(in_image)\n",
        "    fe = Dense(1024)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    fe = Dense(512)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    fe = Dense(256)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    out = Dense(1, activation='sigmoid')(fe)\n",
        "    model = Model(in_image, out)\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5) \n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "bilA8jmcNJo5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = define_discriminator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-jPEhzZNJsY",
        "outputId": "e73d11cb-4b50-4e53-936b-b6309fb914c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_generator(latent_dim): \n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    in_lat = Input(shape=(latent_dim,)) \n",
        "    gen = Dense(256, kernel_initializer=init)(in_lat)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(512, kernel_initializer=init)(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(1024, kernel_initializer=init)(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(28 * 28 * 1, kernel_initializer=init)(gen)\n",
        "    out_layer = Activation('tanh')(gen)\n",
        "    out_layer = Reshape((28, 28, 1))(gen)\n",
        "    model = Model(in_lat, out_layer)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "wrZ4W6nkNJ2s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = define_generator(100)"
      ],
      "metadata": {
        "id": "hGLf8LJdNRwp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_gan(g_model, d_model):\n",
        "    d_model.trainable = False\n",
        "    gan_output = d_model(g_model.output)\n",
        "    model = Model(g_model.input, gan_output)\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Q0dgvBVoNR0D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan_model = define_gan(generator, discriminator)"
      ],
      "metadata": {
        "id": "Ip6BeSomNSAi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g_model, d_model, gan_model, X_train, latent_dim, n_epochs=100, n_batch=64):\n",
        "    bat_per_epo = int(X_train.shape[0] / n_batch)\n",
        "    n_steps = bat_per_epo * n_epochs\n",
        "    for i in range(n_steps):\n",
        "        X_real, y_real = generate_real_samples(X_train, n_batch)\n",
        "        d_loss_r, d_acc_r = d_model.train_on_batch(X_real, y_real)\n",
        "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
        "        d_loss_f, d_acc_f = d_model.train_on_batch(X_fake, y_fake)\n",
        "        z_input = generate_latent_points(latent_dim, n_batch) \n",
        "        y_gan = ones((n_batch, 1)) \n",
        "        g_loss, g_acc = gan_model.train_on_batch(z_input, y_gan)\n",
        "        print('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % (i+1, d_loss_r,d_acc_r, d_loss_f,d_acc_f, g_loss,g_acc))\n",
        "        if (i+1) % (bat_per_epo * 1) == 0:\n",
        "            summarize_performance(i, g_model, latent_dim)"
      ],
      "metadata": {
        "id": "3i21lvGjNW0s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "train(generator, discriminator, gan_model, X_train, latent_dim, n_epochs=5, n_batch=10)\n",
        "#model = load_model('model_18740.h5')\n",
        "latent_dim = 100\n",
        "n_examples = 100\n",
        "latent_points = generate_latent_points(latent_dim, n_examples)\n",
        "X  = model.predict(latent_points)\n",
        "X = (X + 1) / 2.0\n",
        "save_plot(X, n_examples)"
      ],
      "metadata": {
        "id": "s3aHepW2XhTZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}